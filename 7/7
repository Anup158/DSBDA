
# Tokenization
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

from nltk import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

text = 'Real madrid is set to win the UCL for the season . Benzema might win Balon dor . Salah might be the runner up'

# Sentence tokenization
tokens_sents = sent_tokenize(text)
print("Sentence tokens:", tokens_sents)

# Word tokenization
tokens_words = word_tokenize(text)
print("Word tokens:", tokens_words)

# Stemming
from nltk.stem import PorterStemmer
stem = []
ps = PorterStemmer()
for i in tokens_words:
    stem_word = ps.stem(i)
    stem.append(stem_word)
print("Stemmed words:", stem)

# Lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in stem])
print("Lemmatized sentence:", lemmatized_output)

leme = [lemmatizer.lemmatize(i) for i in stem]
print("Lemmatized words:", leme)

# Part of Speech Tagging
print("Parts of Speech:", nltk.pos_tag(leme))

# Stop Words Removal
sw_nltk = stopwords.words('english')
print("Stopwords (English):", sw_nltk)

words = [word for word in text.split() if word.lower() not in sw_nltk]
new_text = " ".join(words)
print("Text after stop word removal:", new_text)
